{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Pytorch model to OpenVino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import hashlib\n",
    "import tempfile\n",
    "import requests\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "# for example we are using the Resnet18\n",
    "from torchvision.models.resnet import resnet18 as net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define helper functions\n",
    "def fetch(url):\n",
    "  # efficient loading of URLS\n",
    "  fp = os.path.join(tempfile.gettempdir(), hashlib.md5(url.encode('utf-8')).hexdigest())\n",
    "  if os.path.isfile(fp) and os.stat(fp).st_size > 0:\n",
    "    with open(fp, \"rb\") as f:\n",
    "      dat = f.read()\n",
    "  else:\n",
    "    print(\"fetching\", url)\n",
    "    dat = requests.get(url).content\n",
    "    with open(fp+\".tmp\", \"wb\") as f:\n",
    "      f.write(dat)\n",
    "    os.rename(fp+\".tmp\", fp)\n",
    "  return dat\n",
    "\n",
    "def get_image_net_labels():\n",
    "  with open(\"labels.txt\", 'r') as f:\n",
    "    labels_map = [x.split(sep=' ', maxsplit=1)[-1].strip()[10:] for x in f]\n",
    "  return labels_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = get_image_net_labels()\n",
    "# load the model by downloading pretrained weights\n",
    "model = net(pretrained=True)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the image transformations to apply to each image\n",
    "image_transformations = transforms.Compose([\n",
    "    transforms.Resize(256),                               # resize to a (256,256) image\n",
    "    transforms.CenterCrop(224),                           # crop centre part of image to get (244, 244) grid\n",
    "    transforms.ToTensor(),                                # convert to tensor\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),    # normalise image according to imagenet valuess\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add URL below for simple run\n",
    "url = \"some/url/to/some/image.jpg\"\n",
    "image = Image.open(io.BytesIO(fetch(url)))  # load any image\n",
    "x = image_transformations(image) # [3, H, W]\n",
    "x = x.view(1, *x.size())\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 results:\n",
      "===============\n",
      "tensor(27.3914) -- cheeseburger\n",
      "tensor(17.5533) -- bagel,_beigel\n",
      "tensor(15.8071) -- hotdog,_hot_dog,_red_hot\n",
      "tensor(15.2610) -- guacamole\n",
      "tensor(13.3716) -- French_loaf\n",
      "tensor(12.9023) -- mushroom\n",
      "tensor(12.6587) -- meat_loaf,_meatloaf\n",
      "tensor(12.4445) -- bolete\n",
      "tensor(12.3000) -- broccoli\n",
      "tensor(11.9797) -- bakery,_bakeshop,_bakehouse\n"
     ]
    }
   ],
   "source": [
    "# keep the top 10 scores\n",
    "number_top = 10\n",
    "probs, indices = torch.topk(out, number_top)\n",
    "print(f\"Top {number_top} results:\")\n",
    "print(\"===============\")\n",
    "for p, i in zip(probs[0], indices[0]):\n",
    "    print(p, \"--\", labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "Now we are converting this pytorch model to ONNX model\n",
      "See in the folder we have a resnet18.onnx file\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# convert to ONNX\n",
    "print(\"-\"*70)\n",
    "print(\"Now we are converting this pytorch model to ONNX model\")\n",
    "torch.onnx._export(model, x, f'resnet18.onnx', export_params=True)\n",
    "print(f\"See in the folder we have a resnet18.onnx file\")\n",
    "print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX to OpenVino\n",
    "\n",
    "For this piece please consult the README since it has CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.inference_engine import IECore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fp32/resnet18.xml', 'fp32/resnet18.bin')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xml = \"fp32/resnet18.xml\"\n",
    "model_bin = os.path.splitext(model_xml)[0] + \".bin\"\n",
    "model_xml, model_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plugin initialization for specified device and load extensions library if specified.\n",
    "print(\"Creating Inference Engine...\")\n",
    "ie = IECore()\n",
    "\n",
    "# Read IR\n",
    "print(\"Loading network\")\n",
    "net = ie.read_network(args.model, os.path.splitext(args.model)[0] + \".bin\")\n",
    "\n",
    "print(\"Loading IR to the plugin...\")\n",
    "exec_net = ie.load_network(network=net, device_name=\"CPU\", num_requests=2)\n",
    "print(f\"exec_net: {exec_net}\")\n",
    "print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numpy to pass this to the IECore\n",
    "x = x.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a bit tricky. So the input to the model is the input from ONNX graph\n",
    "# IECore makes a networkX graph of the \"computation graph\" and when we run .infer\n",
    "# it passes it through. If you are unsure of what to pass you can always check the\n",
    "# <model>.xml file. In case of pytorch models the value \"input.1\" is the usual\n",
    "# suspect. Happy Hunting!\n",
    "out = exec_net.infer(inputs={\"input.1\": x})\n",
    "\n",
    "# the output looks like this {\"node_id\": array()} so we simply load the output\n",
    "out = list(out.values())[0]\n",
    "print(\"Output Shape:\", out.shape)\n",
    "print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the top 10 scores\n",
    "out = out[0]\n",
    "number_top = 10\n",
    "indices = np.argsort(out, -1)[::-1][:number_top]\n",
    "probs = out[indices]\n",
    "print(f\"Top {number_top} results:\")\n",
    "print(\"===============\")\n",
    "for p, i in zip(probs, indices):\n",
    "print(p, \"--\", labels[i])\n",
    "print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Using this method you can convert any arbitrary pytorch model to OpenVino!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intel-pytorch-onnx-openvino",
   "language": "python",
   "name": "intel-pytorch-onnx-openvino"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
